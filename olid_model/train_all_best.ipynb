{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910ba9b-0c08-48fe-8981-b3fedef8a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb7c92-25e5-425c-b636-810649cc80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Charger les donn√©es OLID ===\n",
    "df = pd.read_csv(\"../datasets/training-v1/offenseval-training-v1.tsv\", sep=\"\\t\", header=None)\n",
    "df.columns = [\"id\", \"text\", \"label_A\", \"label_B\", \"label_C\"]\n",
    "\n",
    "# Encode les labels pour chaque task\n",
    "def encode_labels(df):\n",
    "    df = df.copy()\n",
    "    df[\"label_A_enc\"] = df[\"label_A\"].map({\"NOT\": 0, \"OFF\": 1})\n",
    "    df[\"label_B_enc\"] = df[\"label_B\"].map({\"UNT\": 0, \"TIN\": 1})\n",
    "    df[\"label_C_enc\"] = df[\"label_C\"].map({\"IND\": 0, \"GRP\": 1, \"OTH\": 2})\n",
    "    return df\n",
    "\n",
    "df = encode_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f01a6d-712b-4fc3-b1a8-660484acf59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = {\n",
    "    \"A\": \"roberta-base\",             # Task A: general model\n",
    "    \"B\": \"GroNLP/hateBERT\",          # Task B: fine-tuned with loss weighting\n",
    "    \"C\": \"GroNLP/hateBERT\"           # Task C: basic hateBERT\n",
    "}\n",
    "NUM_LABELS = {\"A\": 2, \"B\": 2, \"C\": 3}# === 3. Pr√©parer les datasets HuggingFace ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc12f94-a053-4285-be6c-9f26a004f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Prepare dataset ===\n",
    "def prepare_dataset(df, task):\n",
    "    if task == \"A\":\n",
    "        df_task = df.dropna(subset=[\"label_A_enc\"])\n",
    "        labels = df_task[\"label_A_enc\"].tolist()\n",
    "    elif task == \"B\":\n",
    "        df_task = df[df[\"label_A\"] == \"OFF\"].dropna(subset=[\"label_B_enc\"])\n",
    "        labels = df_task[\"label_B_enc\"].tolist()\n",
    "    elif task == \"C\":\n",
    "        df_task = df[(df[\"label_A\"] == \"OFF\") & (df[\"label_B\"] == \"TIN\")].dropna(subset=[\"label_C_enc\"])\n",
    "        labels = df_task[\"label_C_enc\"].tolist()\n",
    "\n",
    "    texts = df_task[\"text\"].tolist()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAMES[task], use_fast=True)\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long).tolist()\n",
    "    })\n",
    "\n",
    "    return dataset.train_test_split(test_size=0.2, seed=42), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79c614-f825-4860-a1bb-77ab49267ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compute class weights for weighted loss ===\n",
    "def compute_class_weights(labels, num_labels, task=None):\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.arange(num_labels), y=labels)\n",
    "\n",
    "    # Optional boost for Task C\n",
    "    if task == \"C\":\n",
    "        class_weights[1] *= 2.0  # GRP\n",
    "        class_weights[2] *= 3.0  # OTH\n",
    "\n",
    "    return torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318ec8c-d2b0-4cb8-8419-3ddb095adb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFocalLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.to(self.args.device) if class_weights is not None else None\n",
    "        self.gamma = 2.0\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        ce_loss = torch.nn.functional.cross_entropy(logits, labels, weight=self.class_weights, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "        return (focal_loss, outputs) if return_outputs else focal_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216427ed-f540-4e9b-b59a-fde8d33e5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Metrics ===\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
    "    return {\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        \"accuracy\": report[\"accuracy\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335039bc-41b2-458d-acc8-bccbaccd0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Entra√Ænement d'un mod√®le pour chaque task ===\n",
    "# === Training per task ===\n",
    "def train_task(task, resume=True):\n",
    "    print(f\"\\nüß† Starting training for Task {task}\")\n",
    "\n",
    "    # Choose model\n",
    "    if task == \"A\":\n",
    "        model_name = \"roberta-base\"\n",
    "    else:\n",
    "        model_name = \"GroNLP/hateBERT\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    dataset, labels = prepare_dataset(df, task)\n",
    "\n",
    "    output_dir = f\"./results_{task}_{model_name.split('/')[-1]}\"\n",
    "    logging_dir = f\"./logs_{task}_{model_name.split('/')[-1]}\"\n",
    "\n",
    "    checkpoint_path = None\n",
    "    if resume and os.path.isdir(output_dir):\n",
    "        checkpoints = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint\")]\n",
    "        if checkpoints:\n",
    "            checkpoint_path = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n",
    "            print(f\"üîÅ Resuming from checkpoint: {checkpoint_path}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No checkpoint found ‚Äî starting from scratch.\")\n",
    "    else:\n",
    "        print(\"üÜï Starting training from scratch.\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint_path if checkpoint_path else model_name,\n",
    "        num_labels=NUM_LABELS[task]\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        logging_dir=logging_dir,\n",
    "        num_train_epochs=4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        save_strategy=\"steps\" if task == \"B\" else \"epoch\",\n",
    "        save_steps=500 if task == \"B\" else None,\n",
    "        eval_strategy=\"steps\" if task == \"B\" else \"epoch\",\n",
    "        eval_steps=500 if task == \"B\" else None,\n",
    "        logging_steps=100,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        label_smoothing_factor=0.1 if task == \"B\" else 0.0,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        logging_first_step=True,\n",
    "        disable_tqdm=False,\n",
    "        greater_is_better=True,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    if task == \"B\":\n",
    "        class_weights = compute_class_weights(labels, NUM_LABELS[task], task=task)\n",
    "        trainer = WeightedFocalLossTrainer(\n",
    "            class_weights=class_weights,\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=checkpoint_path if checkpoint_path else None)\n",
    "    trainer.save_model(f\"./best_model_{task}_{model_name.split('/')[-1]}\")\n",
    "    print(f\"‚úÖ Task {task} training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9484f-547d-4c43-bbad-8bd6c5773a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Lancer les trois trainings ===\n",
    "for task in [\"A\", \"B\", \"C\"]:\n",
    "    train_task(task, resume=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
