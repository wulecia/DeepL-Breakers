{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DeepLearning' from '/Users/claradelandre/Desktop/EPFL/MA2/DeepLearning/Projet/DeepL-Breakers/DeepLearning.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from DataReader import DataReader\n",
    "from Preprocessor import Preprocessor\n",
    "from Vectorizer import Vectorizer\n",
    "from DeepLearning import DeepLearner\n",
    "\n",
    "import importlib\n",
    "import Classifier\n",
    "import DeepLearning\n",
    "importlib.reload(Classifier)\n",
    "importlib.reload(DeepLearning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Evaluation Function\n",
    "def evaluate_model(model, x, y_true, model_type='sklearn', class_num=2):\n",
    "    if model_type == 'sklearn':\n",
    "        y_pred = model.predict(x)\n",
    "    elif model_type == 'pytorch':\n",
    "        y_pred = model.predict(x)  # Must be implemented in DeepLearner\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"precision\": precision_score(y_true, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "# üß™ Function to load models and evaluate on a subtask\n",
    "def evaluate_subtask(letter, class_num):\n",
    "    print(f\"\\nüîç Evaluating Subtask {letter}\")\n",
    "    dr = DataReader(f'../datasets/training-v1/offenseval-training-v1.tsv', letter)\n",
    "    data, labels = dr.get_labelled_data()\n",
    "    data, labels = dr.shuffle(data, labels, 'random')\n",
    "    tr_data, tst_data, y_tr, y_tst = train_test_split(data, labels, test_size=0.3)\n",
    "\n",
    "    preprocessor = Preprocessor(('remove_stopwords', 'lemmatize'))\n",
    "    vectorizer = Vectorizer('count')\n",
    "    tst_clean = preprocessor.clean(tst_data)\n",
    "    x_tst_vec = vectorizer.vectorize(tst_clean)\n",
    "\n",
    "    model_names = ['RandomForest', 'LogisticRegression', 'NaiveBayes', 'KNN', 'SVC', 'LSTM', 'CNN']\n",
    "    results = {}\n",
    "\n",
    "    for i, name in enumerate(model_names[:-2]):\n",
    "        clf = Classifier.Classifier(name)  # ‚úÖ Recreate Classifier object\n",
    "        clf.load(f\"../saved_models/subtask{letter}_model_{i}.joblib\")\n",
    "        results[name] = evaluate_model(clf, x_tst_vec, y_tst)\n",
    "\n",
    "    # LSTM\n",
    "    lstm = DeepLearner(tr_data, y_tr, vocab_length=vectorizer.vocab_length, model_type='LSTM')\n",
    "    lstm.load(f\"../saved_models/subtask{letter}_lstm.pth\")\n",
    "    results['LSTM'] = evaluate_model(lstm, tst_clean, y_tst, model_type='pytorch')\n",
    "\n",
    "    # CNN\n",
    "    cnn = DeepLearner(tr_data, y_tr, vocab_length=vectorizer.vocab_length, model_type='CNN')\n",
    "    cnn.load(f\"../saved_models/subtask{letter}_cnn.pth\")\n",
    "    results['CNN'] = evaluate_model(cnn, tst_clean, y_tst, model_type='pytorch')\n",
    "\n",
    "    return model_names, results\n",
    "\n",
    "# üìä Plotting function\n",
    "def plot_metric(results_dict, metric, subtask, model_names):\n",
    "    values = [results_dict[model][metric] for model in model_names]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(model_names, values, color='mediumseagreen')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.title(f\"{metric.capitalize()} per Model ‚Äì Subtask {subtask}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating Subtask A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Data: 13241it [00:00, 196456.85it/s]\n",
      "Tokenization: 3972it [00:00, 6129.00it/s]0<?, ?it/s]\n",
      "Stopwords Removal: 3972it [00:00, 76963.62it/s]1,  1.54it/s]\n",
      "Lemmatization: 3972it [00:02, 1934.42it/s]\n",
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-790776936bc7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ‚úÖ Evaluate all subtasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_subtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_subtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_subtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f50307b44019>\u001b[0m in \u001b[0;36mevaluate_subtask\u001b[0;34m(letter, class_num)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ‚úÖ Recreate Classifier object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../saved_models/subtask{letter}_model_{i}.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tst_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f50307b44019>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, x, y_true, model_type, class_num)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sklearn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sklearn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Must be implemented in DeepLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/MA2/DeepLearning/Projet/DeepL-Breakers/Classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, tst_data)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Evaluate all subtasks\n",
    "model_names, results_A = evaluate_subtask('A', 2)\n",
    "_, results_B = evaluate_subtask('B', 2)\n",
    "_, results_C = evaluate_subtask('C', 3)\n",
    "\n",
    "# üìà Plot everything\n",
    "for metric in ['accuracy', 'f1', 'precision', 'recall']:\n",
    "    plot_metric(results_A, metric, 'A', model_names)\n",
    "    plot_metric(results_B, metric, 'B', model_names)\n",
    "    plot_metric(results_C, metric, 'C', model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def results_to_df(results_dict, subtask):\n",
    "    df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "    df['model'] = df.index\n",
    "    df['subtask'] = subtask\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df_A = results_to_df(results_A, 'A')\n",
    "df_B = results_to_df(results_B, 'B')\n",
    "df_C = results_to_df(results_C, 'C')\n",
    "df_all = pd.concat([df_A, df_B, df_C])\n",
    "df_all.to_csv(\"compare_models_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offenseval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
