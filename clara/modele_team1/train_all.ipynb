{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe3ae63-5325-4a6d-bedf-e8b51065111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d28f3eb-a77e-428e-a1b1-92f684cc2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Charger les donnÃ©es OLID ===\n",
    "df = pd.read_csv(\"../datasets/training-v1/offenseval-training-v1.tsv\", sep=\"\\t\", header=None)\n",
    "df.columns = [\"id\", \"text\", \"label_A\", \"label_B\", \"label_C\"]\n",
    "\n",
    "# Encode les labels pour chaque task\n",
    "def encode_labels(df):\n",
    "    df = df.copy()\n",
    "    df[\"label_A_enc\"] = df[\"label_A\"].map({\"NOT\": 0, \"OFF\": 1})\n",
    "    df[\"label_B_enc\"] = df[\"label_B\"].map({\"UNT\": 0, \"TIN\": 1})\n",
    "    df[\"label_C_enc\"] = df[\"label_C\"].map({\"IND\": 0, \"GRP\": 1, \"OTH\": 2})\n",
    "    return df\n",
    "\n",
    "df = encode_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e09e1f-15f3-4aea-8ebc-afd27af5befc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef prepare_dataset(df, task):\\n    if task == \"A\":\\n        df_task = df.copy()\\n        df_task = df_task.dropna(subset=[\"label_A_enc\"])\\n    elif task == \"B\":\\n        df_task = df[df[\"label_A\"] == \"OFF\"].copy()\\n        df_task = df_task.dropna(subset=[\"label_B_enc\"])\\n    elif task == \"C\":\\n        df_task = df[(df[\"label_A\"] == \"OFF\") & (df[\"label_B\"] == \"TIN\")].copy()\\n        df_task = df_task.dropna(subset=[\"label_C_enc\"])\\n\\n    # âž• Mini jeu pour test rapide\\n    df_task = df_task.sample(n=100, random_state=42)\\n    texts = df_task[\"text\"].tolist()\\n\\n    if task == \"A\":\\n        labels = df_task[\"label_A_enc\"].tolist()\\n    elif task == \"B\":\\n        labels = df_task[\"label_B_enc\"].tolist()\\n    elif task == \"C\":\\n        labels = df_task[\"label_C_enc\"].tolist()\\n\\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n    encodings = tokenizer(texts, truncation=True, padding=True)\\n\\n    dataset = Dataset.from_dict({\\n        \"input_ids\": encodings[\"input_ids\"],\\n        \"attention_mask\": encodings[\"attention_mask\"],\\n        \"labels\": [int(l) for l in labels]\\n    })\\n\\n    return dataset.train_test_split(test_size=0.2, seed=42)\\n    '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 2. Choix du modÃ¨le ===\n",
    "MODEL_NAME = \"roberta-large\"\n",
    "NUM_LABELS = {\"A\": 2, \"B\": 2, \"C\": 3}# === 3. PrÃ©parer les datasets HuggingFace ===\n",
    "\n",
    "def prepare_dataset(df, task):\n",
    "    if task == \"A\":\n",
    "        df_task = df.copy()\n",
    "        df_task = df_task.dropna(subset=[\"label_A_enc\"])\n",
    "        labels = df_task[\"label_A_enc\"].tolist()\n",
    "    elif task == \"B\":\n",
    "        df_task = df[df[\"label_A\"] == \"OFF\"].copy()\n",
    "        df_task = df_task.dropna(subset=[\"label_B_enc\"])\n",
    "        labels = df_task[\"label_B_enc\"].tolist()\n",
    "    elif task == \"C\":\n",
    "        df_task = df[(df[\"label_A\"] == \"OFF\") & (df[\"label_B\"] == \"TIN\")].copy()\n",
    "        df_task = df_task.dropna(subset=[\"label_C_enc\"])\n",
    "        labels = df_task[\"label_C_enc\"].tolist()\n",
    "\n",
    "    texts = df_task[\"text\"].tolist()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"labels\": [int(l) for l in labels]\n",
    "    })\n",
    "\n",
    "    return dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "\"\"\"\n",
    "def prepare_dataset(df, task):\n",
    "    if task == \"A\":\n",
    "        df_task = df.copy()\n",
    "        df_task = df_task.dropna(subset=[\"label_A_enc\"])\n",
    "    elif task == \"B\":\n",
    "        df_task = df[df[\"label_A\"] == \"OFF\"].copy()\n",
    "        df_task = df_task.dropna(subset=[\"label_B_enc\"])\n",
    "    elif task == \"C\":\n",
    "        df_task = df[(df[\"label_A\"] == \"OFF\") & (df[\"label_B\"] == \"TIN\")].copy()\n",
    "        df_task = df_task.dropna(subset=[\"label_C_enc\"])\n",
    "\n",
    "    # âž• Mini jeu pour test rapide\n",
    "    df_task = df_task.sample(n=100, random_state=42)\n",
    "    texts = df_task[\"text\"].tolist()\n",
    "\n",
    "    if task == \"A\":\n",
    "        labels = df_task[\"label_A_enc\"].tolist()\n",
    "    elif task == \"B\":\n",
    "        labels = df_task[\"label_B_enc\"].tolist()\n",
    "    elif task == \"C\":\n",
    "        labels = df_task[\"label_C_enc\"].tolist()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"labels\": [int(l) for l in labels]\n",
    "    })\n",
    "\n",
    "    return dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d562a9-2de3-4fc2-9a13-fc42a3a5a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Fonction mÃ©trique ===\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
    "    return {\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        \"accuracy\": report[\"accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71c537d8-c6ee-411c-9fc8-62b41757de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. EntraÃ®nement d'un modÃ¨le pour chaque task ===\n",
    "def train_task(task):\n",
    "    print(f\"\\nðŸ“˜ Training Task {task}\")\n",
    "    task_data = prepare_dataset(df, task)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS[task])\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_task_{task}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,  # => sauvegarde un checkpoint toutes les 500 *mini-batches*\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        logging_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model=\"f1\",\n",
    "    )\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_task_{task}\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        disable_tqdm=False,\n",
    "        report_to=\"none\"\n",
    "    )\"\"\"\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=task_data[\"train\"],\n",
    "        eval_dataset=task_data[\"test\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(f\"./best_model_task_{task}\")\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"\\nðŸ“Š Evaluation Results for Task {task}:\")\n",
    "    print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db613508-2b94-419d-94ee-f9a1e7903a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“˜ Training Task A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3972' max='3972' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3972/3972 2:04:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.538677</td>\n",
       "      <td>0.780708</td>\n",
       "      <td>0.788520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.475375</td>\n",
       "      <td>0.786775</td>\n",
       "      <td>0.796073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.378200</td>\n",
       "      <td>0.480274</td>\n",
       "      <td>0.800156</td>\n",
       "      <td>0.798338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.539294</td>\n",
       "      <td>0.798261</td>\n",
       "      <td>0.800604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.485888</td>\n",
       "      <td>0.790683</td>\n",
       "      <td>0.796828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>0.726252</td>\n",
       "      <td>0.800527</td>\n",
       "      <td>0.800227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.389200</td>\n",
       "      <td>0.585767</td>\n",
       "      <td>0.800383</td>\n",
       "      <td>0.802115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='166' max='166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [166/166 02:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluation Results for Task A:\n",
      "{'eval_loss': 0.7262523174285889, 'eval_f1': 0.800527455096651, 'eval_accuracy': 0.800226586102719, 'eval_runtime': 153.4737, 'eval_samples_per_second': 17.254, 'eval_steps_per_second': 1.082, 'epoch': 3.0}\n",
      "\n",
      "ðŸ“˜ Training Task B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 32:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.342718</td>\n",
       "      <td>0.844425</td>\n",
       "      <td>0.894318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.361024</td>\n",
       "      <td>0.844425</td>\n",
       "      <td>0.894318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluation Results for Task B:\n",
      "{'eval_loss': 0.34271836280822754, 'eval_f1': 0.8444252058679174, 'eval_accuracy': 0.8943181818181818, 'eval_runtime': 44.8174, 'eval_samples_per_second': 19.635, 'eval_steps_per_second': 1.227, 'epoch': 3.0}\n",
      "\n",
      "ðŸ“˜ Training Task C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1164' max='1164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1164/1164 28:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>0.738505</td>\n",
       "      <td>0.676861</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.853745</td>\n",
       "      <td>0.684315</td>\n",
       "      <td>0.722938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluation Results for Task C:\n",
      "{'eval_loss': 0.8537445068359375, 'eval_f1': 0.684315022749301, 'eval_accuracy': 0.7229381443298969, 'eval_runtime': 39.5588, 'eval_samples_per_second': 19.616, 'eval_steps_per_second': 1.239, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# === 6. Lancer les trois trainings ===\n",
    "for task in [\"A\", \"B\", \"C\"]:\n",
    "    train_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4239d5-9d8e-4a28-813b-2bb3d096c5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e7612-aa8d-4cf7-a873-7e915a48dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepl)",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
