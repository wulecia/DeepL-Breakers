{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = Path.cwd()\n",
    "root = cwd.parent\n",
    "df = pd.read_parquet(str(root) + \"/paola/data/measuring-hate-speech.parquet\")\n",
    "df = df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])    #remove rows that miss the column text\n",
    "\n",
    "# Define columns\n",
    "numerical_cols = ['sentiment', 'respect', 'insult', 'humiliate', 'status',\n",
    "                  'dehumanize', 'attack_defend', 'hatespeech']\n",
    "binary_cols = ['target_race', 'target_religion', 'target_origin', 'target_gender',\n",
    "               'target_sexuality']\n",
    "\n",
    "# Convert binary to 0/1\n",
    "df[binary_cols] = df[binary_cols].astype(int)\n",
    "\n",
    "# Split\n",
    "train_texts, val_texts, train_y_num, val_y_num, train_y_bin, val_y_bin = train_test_split(\n",
    "    df['text'], df[numerical_cols], df[binary_cols], test_size=0.2, random_state=42\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, targets_num, targets_bin, tokenizer, max_len=128):\n",
    "        self.texts = list(texts)\n",
    "        self.targets_num = torch.tensor(targets_num.values, dtype=torch.float)\n",
    "        self.targets_bin = torch.tensor(targets_bin.values, dtype=torch.float)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'num_targets': self.targets_num[idx],\n",
    "            'bin_targets': self.targets_bin[idx]\n",
    "        }\n",
    "\n",
    "train_ds = HateSpeechDataset(train_texts, train_y_num, train_y_bin, tokenizer)\n",
    "val_ds = HateSpeechDataset(val_texts, val_y_num, val_y_bin, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom model\n",
    "class Paola(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", num_outputs=8, bin_outputs=5):\n",
    "        super(Paola, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_outputs)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, bin_outputs),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        return self.regressor(pooled), self.classifier(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Paola().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn_num = nn.MSELoss()\n",
    "loss_fn_bin = nn.BCELoss()\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets_num = batch['num_targets'].to(device)\n",
    "        targets_bin = batch['bin_targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_num, out_bin = model(input_ids, attention_mask)\n",
    "\n",
    "        loss_num = loss_fn_num(out_num, targets_num)\n",
    "        loss_bin = loss_fn_bin(out_bin, targets_bin)\n",
    "        loss = loss_num + loss_bin\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets_num = batch['num_targets'].to(device)\n",
    "            targets_bin = batch['bin_targets'].to(device)\n",
    "\n",
    "            out_num, out_bin = model(input_ids, attention_mask)\n",
    "            loss_num = loss_fn_num(out_num, targets_num)\n",
    "            loss_bin = loss_fn_bin(out_bin, targets_bin)\n",
    "            loss = loss_num + loss_bin\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 8.5989, Val Loss = 4.2198\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[107], line 21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved checkpoint at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File /Users/colineleteurtre/Library/CloudStorage/OneDrive-Personnel/Documents/cours/EPFL/MA2/deep_learning/DeepL-Breakers/coline/checkpoint.pt cannot be opened.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load checkpoint if it exists\n",
    "checkpoint_path = str(root) + \"/coline/checkpoint.pt\"\n",
    "start_epoch = 0\n",
    "'''\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resumed from epoch {start_epoch}\")\n",
    "'''\n",
    "start_epoch = 0\n",
    "# Continue training from last checkpoint\n",
    "num_epochs = 1  # Add how many epochs you want to run (total)\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Saved checkpoint at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olid = pd.read_csv(str(root) + \"/datasets/test-A/testset-taska.tsv\", sep=\"\\t\")\n",
    "df_olid = df_olid.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA Democrats Support Antifa, Muslim Brotherhood, MS13, ISIS, Pedophilia, Child Trafficking, Taxpayer Funded Abortionâ€™s, Election Fraud, Sedition And Treason!!! #LockThemAllUp #WWG1WGA #QAnon \\u2066@USER URL'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_olid[\"tweet\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 1.0, 'respect': 1.0, 'insult': 1.0, 'humiliate': 1.0, 'status': 1.0, 'dehumanize': 1.0, 'attack_defend': 1.0, 'hatespeech': 1.0}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(df_olid[\"tweet\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "with torch.no_grad():\n",
    "    out_num, out_bin = model(input_ids, attention_mask)\n",
    "out_num[:5], out_bin[:5]\n",
    "\n",
    "out_num_clipped = torch.round(out_num).clamp(1, 5)\n",
    "out_bin_clipped = np.where(out_bin > 0.5, 1, -1)\n",
    "# Now you can interpret this as predicted ordinal scores for each target\n",
    "print(dict(zip(numerical_cols, out_num_clipped[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class Clara(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(Clara, self).__init__()\n",
    "\n",
    "        # Load pretrained BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size  # typically 768\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(hidden_size, 3)  # 3 output classes\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT output\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output  # [batch_size, hidden_size]\n",
    "\n",
    "        # Class logits\n",
    "        logits = self.classifier(cls_output)  # [batch_size, 3]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta = model #paola.model\n",
    "roberta_model = clara.model\n",
    "\n",
    "extended_dataset = alberta('datasets/trial-data/offenseval-trial.txt')\n",
    "\n",
    "num_outputs = 10\n",
    "\n",
    "class Bertrand(torch.nn.Module):\n",
    "    def __init__(self, model_name=\"roberta-base\"):\n",
    "        super(Bertrand, self).__init__()\n",
    "        self.roberta = roberta_model.from_pretrained(model_name)\n",
    "        self.fc = torch.nn.Linear(self.roberta.config.hidden_size + 5, num_outputs)  # Example: 2 classes for classification\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, new_features):\n",
    "        # Pass input through RoBERTa\n",
    "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = roberta_output.pooler_output  # BERT output (CLS token)\n",
    "        \n",
    "        # Concatenate the new features with the RoBERTa output\n",
    "        combined_output = torch.cat((pooled_output, new_features), dim=1)\n",
    "        \n",
    "        # Pass the concatenated output through a fully connected l