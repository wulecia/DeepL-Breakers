import warnings
import importlib.util
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import numpy as np

# Filter out specific warnings only, instead of all warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

class Classifier:
    
    def __init__(self, type, params=None):
        if params is None:
            params = {}
        
        __classifiers__ = {
            'KNN': KNeighborsClassifier,
            'M-NaiveBayes': MultinomialNB,
            'G-NaiveBayes': GaussianNB,
            'SVC': SVC,
            'DecisionTree': DecisionTreeClassifier,
            'RandomForest': RandomForestClassifier,
            'LogisticRegression': LogisticRegression,
            'MLP': MLPClassifier,
            'AdaBoost': AdaBoostClassifier,
            'Bagging': BaggingClassifier
        }
        
        if type not in __classifiers__:
            raise Exception(f"Available Classifiers: {list(__classifiers__.keys())}")
        
        self.classifier = __classifiers__[type]
        self.params = params
        self.model = self.classifier(**self.params)   

    def fit(self, tr_data, tr_labels):
        return self.model.fit(tr_data, tr_labels)

    def predict(self, tst_data):
        return self.model.predict(tst_data)

    def score(self, tst_data, tst_labels):
        return self.model.score(tst_data, tst_labels)

    def tune(self, tr_data, tr_labels, tune_params=None, best_only=False, scoring='f1'):
        if tune_params is None:
            tune_params = self.params
        tuner = GridSearchCV(self.model, tune_params, n_jobs=4, verbose=1, scoring=scoring)
        tuner.fit(tr_data, tr_labels)
        self.model = tuner.best_estimator_
        if best_only:
            return {'score': tuner.best_score_, 'params': tuner.best_params_}
        else:
            param_scores = {}
            results = tuner.cv_results_
            for i, param in enumerate(tuner.cv_results_['params']):
                param_str = ', '.join("{!s}={!r}".format(key, val) for key, val in param.items())
                param_scores[param_str] = {'test_score': results['mean_test_score'][i], 'train_score': results['mean_train_score'][i]}
            return param_scores
    
    def get_model(self):
        if hasattr(self, 'model'):
            return self.model
        else:
            raise Exception('Model has not been created yet.')

    def test_and_plot(self, tst_data, tst_labels, class_num=2):
        tst_data = np.array(tst_data)
        tst_labels = np.array(tst_labels).reshape(-1, 1)
        predicted_tst_labels = self.model.predict(tst_data)
        conf = np.zeros([class_num, class_num])
        confnorm = np.zeros([class_num, class_num])
        for i in range(tst_data.shape[0]):
            j = tst_labels[i, :]
            k = predicted_tst_labels[i]
            conf[j, k] = conf[j, k] + 1
        for i in range(class_num):
            confnorm[i, :] = conf[i, :] / np.sum(conf[i, :])
        self._confusion_matrix(confnorm, labels=[i for i in range(class_num)])
        return self.model.score(tst_data, tst_labels)

    def _confusion_matrix(self, cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):
        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title)
        plt.colorbar()
        tick_marks = np.arange(len(labels))
        plt.xticks(tick_marks, labels, rotation=45)
        plt.yticks(tick_marks, labels)
        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label')
        plt.show()
